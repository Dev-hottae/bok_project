{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import nltk\n",
    "from sklearn.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBC():\n",
    "    def __init__(self):\n",
    "        self.mails = []\n",
    "        self.tokens = set({})\n",
    "        self.tokens_freq = pd.DataFrame([],columns=['token', 'spam', 'ham'], dtype='int32')\n",
    "    # 딕셔너리 형태로 mail input\n",
    "    def input_data(self, mail, mail_class):\n",
    "        self.mails.append({mail_class: mail})\n",
    "    # data 변형기\n",
    "    def data_transformer(self, datas):\n",
    "        word_column = datas.columns[0]\n",
    "        target_column = datas.columns[1]\n",
    "        target_ls = list(set(datas[target_column]))\n",
    "        datas[word_column] = list(map(lambda i : i.split(','), datas[word_column]))\n",
    "        total_ngram = list(itertools.chain(*list(datas[word_column]))) # list로 들어가지 않을 경우\n",
    "        unique_ngram = list(set(total_ngram))\n",
    "        result_df = pd.DataFrame(unique_ngram, columns = [word_column]).set_index(word_column)\n",
    "        for target in target_ls:\n",
    "            this_ngram = list(itertools.chain(*list(datas[datas[target_column] == target][word_column])))\n",
    "            fdist = nltk.FreqDist(this_ngram)\n",
    "            temp_df = pd.DataFrame(list(zip(fdist.keys(), fdist.values())), columns= [word_column, 'count']).set_index(word_column)\n",
    "            result_df[target] = temp_df['count']\n",
    "        self.df = result_df.fillna(0)\n",
    "        return self.df\n",
    "    # 모든 메일 토크나이징\n",
    "    def tokenizer(self):\n",
    "        # 토큰분리\n",
    "        for mail in self.mails:\n",
    "            mail_token = set(list(mail.values())[0].split())\n",
    "            self.tokens = self.tokens.union(mail_token)\n",
    "        # 분류별 토큰 빈도 수 계산\n",
    "        for token in self.tokens:\n",
    "            token_dict = {\"token\": token, 'spam':0, 'ham':0}\n",
    "            for mail in self.mails:\n",
    "                if token in list(mail.values())[0]:\n",
    "                    mail_type = list(mail.keys())[0]\n",
    "                    count = list(mail.values())[0].count(token)\n",
    "                    token_dict[mail_type] += count\n",
    "            self.tokens_freq = self.tokens_freq.append(token_dict, ignore_index=True)\n",
    "    # 확률계산(n=train 횟수))\n",
    "    def fit(self, n):\n",
    "        # 평균을 내기 위한 데이터 저장소\n",
    "        df_columns = self.tokens_freq.columns\n",
    "        for_avg = self.tokens_freq.drop(columns=df_columns, axis = 1)\n",
    "        # 데이터 스플릿\n",
    "        for i in range(n):\n",
    "            # 임의의 90% 데이터 추출\n",
    "            train = self.tokens_freq.sample(frac=0.9)\n",
    "            # fit 수행\n",
    "            train['whokish'] = ((train[1])/(sum(train[1])))\n",
    "            train['wdovish'] = ((train[-1])/(sum(train[-1])))\n",
    "            train['score{}'.format(i)] = train['whokish']/train['wdovish']\n",
    "            for_avg = pd.merge(for_avg, train['score{}'.format(i)], left_index=True, right_index=True, suffixes=('_',''), how='outer')\n",
    "            try:\n",
    "                for_avg.drop('score{}_'.format(i), axis=1, inplace=True)\n",
    "            except:\n",
    "                pass\n",
    "        self.tokens_freq['avg_score'] = for_avg.mean(axis=1)\n",
    "        print(self.tokens_freq.head(50))\n",
    "    # 스팸 분류기\n",
    "    def classify(self, mail):\n",
    "        mail_token = mail.split()\n",
    "        p_ham = 0\n",
    "        p_spam = 0\n",
    "        for token in mail_token:\n",
    "            if token in self.tokens:\n",
    "                row = self.tokens_freq.loc[self.tokens_freq['token'] == token]\n",
    "                p_ham += float(row.wham)\n",
    "                p_spam += float(row.wspam)\n",
    "        p_ham = np.exp(p_ham)\n",
    "        p_spam = np.exp(p_spam)\n",
    "        result = p_spam / (p_ham + p_spam)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 의사록 데이터\n",
    "test_data = pd.read_json(r'C:\\Users\\student\\Desktop\\newsdata\\test_ngram_datas.json')\n",
    "test_data['ngram'] = list(map(lambda i : i.split(','), test_data['ngram']))\n",
    "test_data['date'] = list(map(lambda i : i.date(), test_data['date']))\n",
    "test_data = test_data[test_data['date']<= datetime.date(2017,12,31)]\n",
    "# hawkish, dovish 사전 필요\n",
    "# 0으로 나눠지는 값의 의미 = hawkish, dovish 사전에 아예 없음을 의미함\n",
    "# 해당 부분은 드랍하는 것이 맞을 듯\n",
    "def tone_sent(x):\n",
    "    a = 0\n",
    "    b = 0\n",
    "    for ngram in x:\n",
    "        if ngram in ppp[ppp['avg_score']>1.3].index:\n",
    "            a += 1\n",
    "        elif ngram in ppp[ppp['avg_score']<(10/13)].index:\n",
    "            b += 1\n",
    "    try:\n",
    "        return (a-b) / (a+b)\n",
    "    except:\n",
    "        return np.nan\n",
    "test_data['tone'] = list(map(tone_sent, test_data['ngram']))\n",
    "test_data.dropna(inplace=True)\n",
    "# 0은 중립\n",
    "test_data['HD'] = list(map(lambda i : 'H' if i > 0 else 'D' if i < 0 else np.nan, test_data['tone']))\n",
    "test_data.dropna(inplace=True)\n",
    "test_data['H'] = list(map(lambda i : 1 if i == 'H' else 0, test_data['HD']))\n",
    "test_data['D'] = list(map(lambda i : 1 if i == 'D' else 0, test_data['HD']))\n",
    "final_tone = test_data.groupby('date').sum()[['H','D']]\n",
    "final_tone['tone'] = (final_tone['H'] - final_tone['D']) / (final_tone['H'] + final_tone['D'])\n",
    "sr_df = pd.read_json(r'C:\\Users\\student\\Desktop\\newsdata\\rate\\standard_rate.json').set_index('date')\n",
    "final_tone['rate'] = sr_df['rate']\n",
    "corr = final_tone[['tone','rate']].corr(method = 'pearson')\n",
    "print(corr)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(final_tone['tone'])\n",
    "def norm(x):\n",
    "    if x - final_tone['rate'].mean() > 0:\n",
    "        (x - final_tone['rate'].mean()) / (final_tone['rate'].max() - final_tone['rate'].mean())\n",
    "    else:\n",
    "        (x - final_tone['rate'].mean()) / (final_tone['rate'].mean() - final_tone['rate'].min())\n",
    "final_tone['norm_rate'] = list(map(norm, final_tone['rate']))\n",
    "plt.plot(final_tone['norm_rate'])\n",
    "plt.show()"
   ]
  }
 ]
}